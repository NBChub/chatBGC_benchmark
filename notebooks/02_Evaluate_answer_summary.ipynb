{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db549088-ee23-4747-8271-69dbc178943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def evaluate_sql(client, question, answer_sql_expected, answer_sql_LLM, model=\"gpt-4o-mini\"):\n",
    "    content = f\"\"\"I have one question and two sql statements describing the query for that question.\n",
    "One sql statement is a response from LLM and the other is a human curated sql statement.\n",
    "\n",
    "1. Question: {question}\n",
    "\n",
    "2. Human curated sql query: {answer_sql_expected}\n",
    "\n",
    "3. Response sql query from LLM: {answer_sql_LLM}\n",
    "\n",
    "Evaluate whether the LLM generated SQL query is correct or not based on the example sql query from human.\n",
    "Only answer using True or False and do not give the reason.\n",
    "\"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    )\n",
    "\n",
    "    response_text = chat_completion.choices[0].message.content\n",
    "    assert response_text in [\"True\", \"False\"]\n",
    "    if response_text == \"True\":\n",
    "        response = True\n",
    "    elif response_text == \"False\":\n",
    "        response = False\n",
    "    \n",
    "    return response, chat_completion\n",
    "\n",
    "def evaluate_answer(client, question, sql, answer_summary_expected, answer_summary_LLM, model=\"gpt-4o-mini\"):\n",
    "    content = f\"\"\"I have one question, one SQL query, and two statements describing the query result.\n",
    "One statement is a response from LLM, the other is a human curated answer.\n",
    "\n",
    "1. Question: {question}\n",
    "\n",
    "2. SQL query: {sql}\n",
    "\n",
    "3. Human curated answer: {answer_summary_expected}\n",
    "\n",
    "4. Response from LLM: {answer_summary_LLM}\n",
    "\n",
    "Rate the LLM response on a scale of 1 to 5, where:\n",
    "\n",
    "1. Completely Incorrect\n",
    "2. Mostly Incorrect\n",
    "3. Partially Correct\n",
    "4. Mostly Correct\n",
    "5. Completely Correct\n",
    "\n",
    "Start the answer with the rating then provide a detailed explanation for your rating.\n",
    "An example answer would be: \n",
    "Rating 5 (Completely Correct). The answer is correct because ...\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    )\n",
    "\n",
    "    # Extract the rating number using a regular expression\n",
    "    response_text = chat_completion.choices[0].message.content\n",
    "    rating_match = re.search(r'\\b[1-5]\\b', response_text)\n",
    "    \n",
    "    if rating_match:\n",
    "        rating = int(rating_match.group(0))\n",
    "    else:\n",
    "        rating = None\n",
    "        \n",
    "    return rating, response_text, chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5135991-ed92-4464-ba33-f9f7deabd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_dotenv(\"../.env\")  \n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"1.Refined_benchmark_response.tsv\", sep=\"\\t\", index_col=0)\n",
    "df_subset = df.copy()\n",
    "\n",
    "df_evaluation = df.copy()\n",
    "df_evaluation[\"summary_evaluation\"] = None\n",
    "df_evaluation[\"sql_evaluation\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3de0a-3cc2-4bca-be54-f360f6bbcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in df_subset.index:\n",
    "    if df_subset.loc[index, \"sql_query_success\"]:\n",
    "        model = df_subset.loc[index, \"model\"]\n",
    "        question = df_subset.loc[index, \"question\"]\n",
    "        answer_sql_expected = df_subset.loc[index, \"answer_sql_expected\"]\n",
    "        answer_sql_LLM = df_subset.loc[index, \"answer_sql_LLM\"]\n",
    "        answer_summary_expected = df_subset.loc[index, \"answer_summary_expected\"]\n",
    "        answer_summary_LLM = df.loc[index, \"answer_summary_LLM\"]\n",
    "        if \"summary_evaluation\" in df_evaluation.columns:\n",
    "            if type(df_evaluation.loc[index, \"summary_evaluation\"]) is int:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Evaluating answer:\", index)\n",
    "                rating, evaluation_message, evaluation = evaluate_answer(client, question, answer_sql_expected, answer_summary_expected, answer_summary_LLM)\n",
    "                df_evaluation.loc[index, \"evaluator_model\"] = model\n",
    "                df_evaluation.loc[index, \"summary_evaluation_reason\"] = evaluation_message\n",
    "                df_evaluation.loc[index, \"summary_evaluation\"] = rating\n",
    "        if \"sql_evaluation\" in df_evaluation.columns:\n",
    "            if type(df_evaluation.loc[index, \"sql_evaluation\"]) is bool:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Evaluating sql:\", index)\n",
    "                sql_evaluation_result, sql_evaluation = evaluate_sql(client, question, answer_sql_expected, answer_sql_LLM)\n",
    "                df_evaluation.loc[index, \"sql_evaluator_model\"] = model\n",
    "                df_evaluation.loc[index, \"sql_evaluation\"] = sql_evaluation_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
